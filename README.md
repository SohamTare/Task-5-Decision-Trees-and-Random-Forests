# Decision-Trees-and-Random-Forests
Decision Trees and Random Forest



## ğŸ¯ Objective
To implement tree-based models like Decision Tree and Random Forest for classification tasks using the Breast Cancer Wisconsin dataset.

---

## ğŸ“‚ Dataset
- **Source**: [Breast Cancer Wisconsin Dataset - Kaggle](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data)
- **Target**: `diagnosis` (M = malignant, B = benign)

---

## ğŸ”§ Tools & Libraries
- Python
- scikit-learn
- matplotlib, seaborn
- pandas, numpy

---

## ğŸ“Œ Steps Implemented

### âœ… 1. Data Preprocessing
- Loaded and cleaned dataset
- Dropped ID and unnamed columns
- Encoded target label (M = 1, B = 0)

### ğŸŒ³ 2. Decision Tree Classifier
- Trained a basic decision tree with max depth control
- Visualized the tree using `plot_tree`
- Evaluated model using classification report

### ğŸŒ² 3. Random Forest Classifier
- Trained with 100 estimators
- Compared performance against Decision Tree
- Used classification metrics for evaluation

### ğŸ” 4. Feature Importance
- Ranked and visualized top 10 features impacting prediction

### ğŸ“ˆ 5. Cross-validation
- Performed 5-fold CV for model validation
- Reported mean accuracy for robustness

---

## ğŸ“Š Results

| Model          | Accuracy | Notes                      |
|----------------|----------|----------------------------|
| Decision Tree  | ~92%     | Slightly prone to overfit  |
| Random Forest  | ~96%     | More robust and accurate   |

---

## ğŸ” Observations
- Random Forest performs better due to ensembling.
- Feature importance gives insights on which variables drive predictions.
- Tree depth control helps mitigate overfitting in Decision Trees.

---

## ğŸ“ Files
- `decision_tree_random_forest.py`: Main Python script
- `data.csv`: Dataset file (download from Kaggle)
- `README.md`: This file
